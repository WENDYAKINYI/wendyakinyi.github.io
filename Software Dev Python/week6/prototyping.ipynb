{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Assignment - Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load the data into a pandas dataframe (you may get a warning, you can get rid of it by setting low_memory=False). \n",
    "\n",
    "### Print the first 10 rows and print a random sampling of the rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the DataFrame:\n",
      "     status  bed bath  acre_lot           city        state  zip_code  \\\n",
      "0  for_sale  3.0  2.0      0.12       Adjuntas  Puerto Rico     601.0   \n",
      "1  for_sale  4.0  2.0      0.08       Adjuntas  Puerto Rico     601.0   \n",
      "2  for_sale  2.0  1.0      0.15     Juana Diaz  Puerto Rico     795.0   \n",
      "3  for_sale  4.0  2.0      0.10          Ponce  Puerto Rico     731.0   \n",
      "4  for_sale  6.0  2.0      0.05       Mayaguez  Puerto Rico     680.0   \n",
      "5  for_sale  4.0  3.0      0.46  San Sebastian  Puerto Rico     612.0   \n",
      "6  for_sale  3.0  1.0      0.20         Ciales  Puerto Rico     639.0   \n",
      "7  for_sale  3.0  2.0      0.08          Ponce  Puerto Rico     731.0   \n",
      "8  for_sale  2.0  1.0      0.09          Ponce  Puerto Rico     730.0   \n",
      "9  for_sale  5.0  3.0      7.46     Las Marias  Puerto Rico     670.0   \n",
      "\n",
      "   house_size prev_sold_date     price  \n",
      "0       920.0            NaN  105000.0  \n",
      "1      1527.0            NaN   80000.0  \n",
      "2       748.0            NaN   67000.0  \n",
      "3      1800.0            NaN  145000.0  \n",
      "4         NaN            NaN   65000.0  \n",
      "5      2520.0            NaN  179000.0  \n",
      "6      2040.0            NaN   50000.0  \n",
      "7      1050.0            NaN   71600.0  \n",
      "8      1092.0            NaN  100000.0  \n",
      "9      5403.0            NaN  300000.0  \n",
      "\n",
      "Random sample of 10 rows from the DataFrame:\n",
      "           status  bed bath  acre_lot           city       state  zip_code  \\\n",
      "1130918  for_sale  4.0  3.0      2.00         Quogue    New York   11959.0   \n",
      "902909   for_sale  3.0  3.0       NaN       Brooklyn    New York   11201.0   \n",
      "1230779  for_sale  NaN  NaN      0.87         Walden    New York   12586.0   \n",
      "702338   for_sale  2.0  2.0       NaN        Whiting  New Jersey    8759.0   \n",
      "1361672  for_sale  3.0  1.0      0.36      Cleveland    New York   13042.0   \n",
      "494241   for_sale  6.0  2.0      0.07       Brooklyn    New York   11207.0   \n",
      "1372712  for_sale  NaN  NaN    137.79         Madrid    New York   13660.0   \n",
      "827241   for_sale  2.0  2.0       NaN  New York City    New York   10031.0   \n",
      "1348693  for_sale  4.0  3.0      0.44     Waterville    New York   13480.0   \n",
      "1152532  for_sale  4.0  4.0      0.44      Bethlehem    New York   12054.0   \n",
      "\n",
      "         house_size prev_sold_date      price  \n",
      "1130918      2542.0            NaN  1695000.0  \n",
      "902909       2470.0     2012-03-30  4999999.0  \n",
      "1230779         NaN     2016-02-26    89000.0  \n",
      "702338          NaN            NaN   105000.0  \n",
      "1361672       846.0            NaN    69900.0  \n",
      "494241          NaN     2018-01-10   950000.0  \n",
      "1372712         NaN            NaN   139900.0  \n",
      "827241        955.0     2006-06-13   785000.0  \n",
      "1348693      4116.0     2002-05-15   239900.0  \n",
      "1152532      3238.0            NaN   599000.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_sample_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file into a pandas DataFrame and prints the first 10 rows and a random sample of rows.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file containing the realtor data.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Load the data into a pandas dataframe. Setting low_memory=False to avoid dtype warning due to mixed types in columns.\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    # Print the first 10 rows to get an initial understanding of the data.\n",
    "    print(\"First 10 rows of the DataFrame:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # Print a random sampling of 10 rows to see diverse entries across the dataset.\n",
    "    print(\"\\nRandom sample of 10 rows from the DataFrame:\")\n",
    "    print(df.sample(10))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the path to the realtor-data.csv file\n",
    "    data_file_path = 'data/realtor-data.csv'\n",
    "\n",
    "    # Call the function to load and display data\n",
    "    load_and_sample_data(data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) You should always check how many null values there are in your data as well as the data types of the data you're working with. Often you will come across data that looks correct but isn't the right data type. \n",
    "\n",
    "### Check the number of null values for every column and check the data types as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column:\n",
      "status                 0\n",
      "bed               216467\n",
      "bath              194206\n",
      "acre_lot          357467\n",
      "city                 191\n",
      "state                  0\n",
      "zip_code             479\n",
      "house_size        450112\n",
      "prev_sold_date    686293\n",
      "price                108\n",
      "dtype: int64\n",
      "\n",
      "Data types of each column:\n",
      "status             object\n",
      "bed                object\n",
      "bath               object\n",
      "acre_lot          float64\n",
      "city               object\n",
      "state              object\n",
      "zip_code          float64\n",
      "house_size        float64\n",
      "prev_sold_date     object\n",
      "price              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_and_inspect_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the data from a CSV file into a pandas DataFrame, and inspect for null values\n",
    "    and data types of each column.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, the path to the CSV file containing the realtor data.\n",
    "    \"\"\"\n",
    "    # Load the data into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "    # Null values check in each column\n",
    "    null_values = df.isnull().sum()\n",
    "    print(\"Number of null values in each column:\")\n",
    "    print(null_values)\n",
    "\n",
    "    # Data types check of each column\n",
    "    data_types = df.dtypes\n",
    "    print(\"\\nData types of each column:\")\n",
    "    print(data_types)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Function call to load the data and inspect for null values and data types\n",
    "  load_and_inspect_data(data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) We have 3 columns that looked right when checking the data but aren't the right data type and we'll correct it. \n",
    "\n",
    "### Cast the columns bed, bath and price to float. Values that cannot be casted to float, like \"hello\" should be turned into NaN. \n",
    "\n",
    "### Check the data types again to make sure the conversion was successfull.\n",
    "\n",
    "\n",
    "\n",
    "### Get a count of the number of NaNs in bed, bath and price columns. \n",
    "\n",
    "### You should get 216535, 194215 and 110 respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated data types of each column:\n",
      "status             object\n",
      "bed               float64\n",
      "bath              float64\n",
      "acre_lot          float64\n",
      "city               object\n",
      "state              object\n",
      "zip_code          float64\n",
      "house_size        float64\n",
      "prev_sold_date     object\n",
      "price             float64\n",
      "dtype: object\n",
      "\n",
      "Count of NaN values in 'bed', 'bath', and 'price' columns:\n",
      "bed      216535\n",
      "bath     194215\n",
      "price       110\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def clean_and_convert_types(df):\n",
    "    \"\"\"\n",
    "    Clean and convert data types of specific columns in the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the pandas DataFrame containing the realtor data.\n",
    "    \"\"\"\n",
    "    # Convert 'bed', 'bath', and 'price' to float, coercing errors to NaN\n",
    "    for column in ['bed', 'bath', 'price']:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    # Check the updated data types\n",
    "    print(\"\\nUpdated data types of each column:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def count_nan_values(df, columns):\n",
    "    \"\"\"\n",
    "    Count the number of NaN values in specified columns of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the pandas DataFrame containing the realtor data.\n",
    "    - columns: list of str, the columns in which to count NaN values.\n",
    "    \"\"\"\n",
    "    nan_counts = df[columns].isnull().sum()\n",
    "    return nan_counts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # data_file_path = 'data/realtor-data.csv'\n",
    " df = pd.read_csv(data_file_path, low_memory=False)\n",
    "\n",
    "# # Clean and convert data types for 'bed', 'bath', and 'price'\n",
    "df = clean_and_convert_types(df)\n",
    "\n",
    "# Get count of NaN values in 'bed', 'bath', and 'price' columns\n",
    "nan_counts = count_nan_values(df, ['bed', 'bath', 'price'])\n",
    "print(\"\\nCount of NaN values in 'bed', 'bath', and 'price' columns:\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Check the number of unique values in the bed, bath and state columns. \n",
    "\n",
    "### You should get 49, 42 and 19 respectively\n",
    "\n",
    "### Print the uniques values for bed, bath and state. What do you notice about the unique values ? \n",
    "\n",
    "### Ans. Further data cleaning steps may be needed to address outliers and anomalies in the bed and bath columns, such as removing or capping extreme values that do not align with typical residential property characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_16676\\3960256315.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(most_common_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in bed: 49\n",
      "Unique values in bed: [  3.   4.   2.   6.   5.   1.   9.   7.   8.  12.  13.  10.  11.  33.\n",
      "  24.  28.  14.  18.  20.  16.  15.  19.  17.  40.  21.  86.  31.  27.\n",
      "  42.  60.  22.  32.  99.  49.  29.  30.  23.  46.  36.  68. 123.  25.\n",
      "  47.  inf  35.  38.  64.  48.  75.]\n",
      "\n",
      "Number of unique values in bath: 42\n",
      "Unique values in bath: [  2.   1.   3.   5.   4.   7.   6.   8.   9.  10.  12.  13.  35.  11.\n",
      "  16.  15.  18.  20.  14.  36.  25.  17.  19.  56.  42.  51.  28. 198.\n",
      "  22.  33.  27.  30.  29.  24.  46.  21. 123.  39.  43.  32.  45.  64.]\n",
      "\n",
      "Number of unique values in state: 19\n",
      "Unique values in state: ['Puerto Rico' 'Virgin Islands' 'Massachusetts' 'Connecticut'\n",
      " 'New Hampshire' 'Vermont' 'New Jersey' 'New York' 'South Carolina'\n",
      " 'Tennessee' 'Rhode Island' 'Virginia' 'Wyoming' 'Maine' 'Georgia'\n",
      " 'Pennsylvania' 'West Virginia' 'Delaware' 'Louisiana']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_unique_values(df, columns):\n",
    "    \"\"\"\n",
    "    Check and print the number of unique values for specified columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the DataFrame to analyze.\n",
    "    - columns: list of str, the column names to check for unique values.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"Number of unique values in {column}: {len(unique_values)}\")\n",
    "        print(f\"Unique values in {column}: {unique_values}\\n\")\n",
    "\n",
    "\n",
    "def handle_missing_values(df, columns):\n",
    "    \"\"\"\n",
    "    Handle missing values in specified columns by dropping rows with NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame, the DataFrame to handle missing values.\n",
    "    - columns: list of str, the column names to check for missing values.\n",
    "\n",
    "    Returns:\n",
    "    - df_cleaned: pandas DataFrame with missing values handled.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        most_common_value = df[column].mode()[0]\n",
    "        df[column].fillna(most_common_value, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Columns to check for unique values\n",
    "    columns_to_check = ['bed', 'bath', 'state']\n",
    "    columns_to_clean = ['bed', 'bath']\n",
    "\n",
    "    # Handle missing values\n",
    "    df_cleaned = handle_missing_values(df, columns_to_clean)\n",
    "    \n",
    "    # Check and print unique values\n",
    "    check_unique_values(df_cleaned, columns_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) We want to see which state has the largest number of properties for sale. \n",
    "\n",
    "### Print a count of the number of properties in each state/territory. \n",
    "\n",
    "### We want to make sure that we're getting unique listings, so drop any duplicate rows and print the count of the number of properties. What do you notice about the number of properties in each state ?\n",
    "\n",
    "### Ans.The stark differences in property counts between states, especially those with very few listings, raise questions about the dataset's quality and completeness. It might be necessary to investigate whether the dataset accurately reflects the real estate market or if there are gaps in the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of properties in each state/territory:\n",
      "state\n",
      "New York          67157\n",
      "New Jersey        32598\n",
      "Connecticut       13753\n",
      "Massachusetts     10052\n",
      "Pennsylvania       9549\n",
      "Maine              4938\n",
      "New Hampshire      3431\n",
      "Rhode Island       3332\n",
      "Puerto Rico        2649\n",
      "Vermont            2544\n",
      "Delaware           1290\n",
      "Virgin Islands      730\n",
      "Virginia              7\n",
      "Georgia               5\n",
      "West Virginia         1\n",
      "Tennessee             1\n",
      "Wyoming               1\n",
      "South Carolina        1\n",
      "Louisiana             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_properties_by_state(df):\n",
    "    \"\"\"\n",
    "    Drop duplicate rows and count the number of properties in each state.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the pandas DataFrame containing the realtor data.\n",
    "    \"\"\"\n",
    "    # Drop duplicate rows to ensure unique listings\n",
    "    unique_df = df.drop_duplicates()\n",
    "\n",
    "    # Count the number of properties in each state\n",
    "    properties_count = unique_df['state'].value_counts()\n",
    "    print(\"Count of properties in each state/territory:\")\n",
    "    print(properties_count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Count properties by state\n",
    "    count_properties_by_state(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) We now want to look for patterns in our data, find the 5 dates when the most houses were sold. What do you notice ?\n",
    "\n",
    "### Ans.The dates with the highest number of houses sold are relatively close together, all within a span of a few months. This could indicate a particularly active period in the real estate market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 dates when the most houses were sold:\n",
      "prev_sold_date\n",
      "2022-04-15    734\n",
      "2022-02-28    554\n",
      "2022-03-31    516\n",
      "2021-10-13    478\n",
      "2022-01-21    433\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def top_dates_most_houses_sold(df, num_dates=5):\n",
    "    \"\"\"\n",
    "    Find the top dates when the most houses were sold.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the pandas DataFrame containing the realtor data.\n",
    "    - num_dates: int, the number of top dates to identify (default is 5).\n",
    "    \"\"\"\n",
    "    # Convert 'prev_sold_date' to datetime format\n",
    "    df['prev_sold_date'] = pd.to_datetime(df['prev_sold_date'])\n",
    "\n",
    "    # Count the number of houses sold on each date\n",
    "    top_dates = df['prev_sold_date'].value_counts().nlargest(num_dates)\n",
    "\n",
    "    print(f\"Top {num_dates} dates when the most houses were sold:\")\n",
    "    print(top_dates)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Find the top dates when the most houses were sold\n",
    "    top_dates_most_houses_sold(df, num_dates=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Now we want to create a simple but effective summary of the properties that are for sale. \n",
    "\n",
    "### Let's create a summary table that contains the average home size and price, every state and each city within a state. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary table of average home size and price for each city within a state:\n",
      "                              house_size         price\n",
      "state          city                                   \n",
      "Connecticut    Andover       1607.180328  2.623527e+05\n",
      "               Ansonia       1840.066372  2.939403e+05\n",
      "               Ashford       1648.345324  2.762310e+05\n",
      "               Avon          2977.006965  6.036860e+05\n",
      "               Barkhamsted   2411.147783  3.866785e+05\n",
      "...                                  ...           ...\n",
      "Virgin Islands Saint Thomas  3483.603448  1.169000e+06\n",
      "Virginia       Cape Charles          NaN  7.100000e+05\n",
      "               Chincoteague          NaN  1.707000e+05\n",
      "West Virginia  Wyoming       1860.000000  6.250000e+04\n",
      "Wyoming        Cody          1935.000000  5.350000e+05\n",
      "\n",
      "[4308 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_summary_table(df):\n",
    "    \"\"\"\n",
    "    Create a summary table with the average home size and price for each city within a state.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the pandas DataFrame containing the realtor data.\n",
    "    \"\"\"\n",
    "    # Ensure numeric columns are treated as such\n",
    "    df['house_size'] = pd.to_numeric(df['house_size'], errors='coerce')\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "\n",
    "    # Group by state and city, then calculate the mean of house_size and price\n",
    "    summary_table = df.groupby(['state', 'city'])[\n",
    "        ['house_size', 'price']].mean()\n",
    "\n",
    "    print(\"Summary table of average home size and price for each city within a state:\")\n",
    "    print(summary_table)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   # Create the summary table\n",
    "    create_summary_table(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
