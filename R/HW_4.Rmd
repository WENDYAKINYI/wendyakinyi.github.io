---
title: "Problem Set 4"
subtitle: "applications of probability theory"
author: "W. Akinyi"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
```

# Introduction

These questions were rendered in R markdown through RStudio (<https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf>, <http://rmarkdown.rstudio.com> ).

Please generate your solutions in R markdown and upload both a knitted doc, docx, or pdf document in addition to the Rmd or Qmd file.
Please put your name in the "author" section in the header.

The questions in this problem set use material from the slides on discrete and continuous probability spaces, slides on parameter estimation for the Binomial and Normal families, and the corresponding `Rmd`'s.


# Question 1

Consider the continuous probability space $(\mathcal{S, M,P})$ defined by $\mathcal{S}=[0,2]$ and $\mathcal{P}$ defined by $f(x)=x/2$ for $x\in [0,2]$ and $f(x)=0$ otherwise (with $\mathcal{M}$ consisting of measurable sets).

## Q1, part 1

(10 points)

Please give the cumulative distribution function $F(t)$ for the density above.
the CDF (F(t)) is obtained by integrating (f(x)) from the lower bound of the domain up to (t):

Since; $[F(t) = \int_{\text{lower bound}}^{t} f(x) dx]$

For $(t < 0), (F(t) = 0)$ because the PDF is $0$ outside the interval $[0,2]$.
For $(0 \leq t \leq 2)$, we integrate $(f(x) = \frac{x}{2})$ from 0 to (t):
So $[F(t) = \int_{0}^{t} \frac{x}{2} dx = \left[ \frac{x^2}{4} \right]_{0}^{t} = \frac{t^2}{4}]$

For $(t > 2), (F(t) = 1)$ because the total probability over the entire sample space is $1$, and our PDF is defined only up to $(x = 2)$.
So, the cumulative distribution function $(F(t))$ based on the given PDF $(f(x) = x/2)$ for $(x \in [0,2])$ is:

$[F(t) = \begin{cases} 0 & \text{for } t < 0, \ \frac{t^2}{4} & \text{for } 0 \leq t \leq 2 ,\ 1 & \text{for } t > 2 \end{cases}]$


## Q1, part 2

(10 points)

Restricting domain of the $F$ to be $[0,2]$, note that $F$ is invertible. Denote the inverse by $F^{-1}$.


Define a random variable $X:[0,1]\rightarrow\mathbb{R}$ on the uniform distribution on $[0,1]$ by setting $X(s)=F^{-1}(s)$. For a given $t_0=X(s_0)\in [0,2]$, what is the probability of the event $\{s:X(s)\leq t_0\}$? You may find that thinking about the values of $s$ for which $s<s_0$ is helpful.

Given $(t_0 = 2\sqrt{s_0})$:

then: $[s_0 = \left(\frac{t_0}{2}\right)^2]$

So, the probability of the event $({s : X(s) \leq t_0}) is (s_0)$, is:

$[P(X(s) \leq t_0) = P(s \leq s_0) = \left(\frac{t_0}{2}\right)^2]$


## Q1, part 3

(10 points)

Consider the probability space defined by the random variable $X$. Please give the density function of this probability space.
Density function of the random variable (X) is represented by the white line So;$(f(x) = x/2) for (x \in [0,2])$ and $ (f(x) = 0)$ .

A large sample from this distribution is given by the following code:

```{r cache=TRUE}
# sampling function
get.x<-function(){
  u<-runif(1)
  return(sqrt(u)*2)
}

# draw sample
set.seed(123456)
sample.x<-replicate(1e6,get.x())

temp<-data.frame(x=sample.x)
ggplot(temp,aes(x=sample.x))+geom_histogram(aes(y=after_stat(density)),bins=50,boundary=min(sample.x))+
  geom_abline(slope=0.5, intercept=0, color="white")

```





# Question 2

(10 points)

The Weibull distributions are a parametrized family of a continuous probability spaces. They are used in models of the time to failure for appliances, for example. The family is supported by R. Information regarding the distributions can be found in the help file for `dweibull`. The file `dat_weibull.RData` contains a sample of size 20 from a Weibull distribution. Below, you will use `nlm` or `optim` to find the maximum likelihood estimate of the shape and scale parameters of the Weibull distribution for these observations.

```{r}


load("dat_weibull.RData") # load the data, assuming it's in the same folder as your Rmd.

# Plot the data, overlaid with a smoothed density estimate and a collection of Weibull densities with different parameters. If these colors aren't clearly distinguishable, you can change add densities one at a time

ggplot(dat,aes(x=x))+geom_histogram(aes(y=after_stat(density)),bins=20)+
  geom_density(color="pink")+
  stat_function(fun=dweibull,args=list(shape=5,scale=2))+
  stat_function(fun=dweibull,args=list("shape"=5,"scale"=1),linetype="dashed")+
  stat_function(fun=dweibull,colour = "orange",args=list(shape=0.5,scale=2))+
  stat_function(fun=dweibull,colour = "orange",args=list("shape"=0.5,"scale"=1),linetype="dashed")+
  stat_function(fun=dweibull,colour = "darkgreen",args=list("shape"=1.5,"scale"=2))+
  stat_function(fun=dweibull,colour = "darkgreen",args=list("shape"=1.5,"scale"=1),linetype="dashed")

# negative log likelihood function for data in "samp", assuming the data are a sample from the Weibull distribution with parameters "shape" in theta[1] and "scale" in theta[2].
negloglik <- function(theta, samp) {
  if (theta[1] <= 0 || theta[2] <= 0) return(Inf) # Ensure parameters are positive
  -sum(dweibull(samp, shape = theta[1], scale = theta[2], log = TRUE))
}
# Your code for numeric optimization here. You can use the plotted Weibull densities above to help you estimate the initial values for the parameters.
#nlm(f=?,p=?,samp=?)$estimate # you may get a warning here. You can use the output as new estimates for the initial values for the parameters. 
#The "optim" function may be more stable.
# optim(par=?,fn=?,samp=?)$par

# Using optim with better initial guesses and parameter constraints
initial_values <- c(shape = 2, scale = 2) # Adjusted initial values
# Defining lower bounds to ensure parameters are positive
lower_bounds <- c(1e-5, 1e-5)

# Optim call with L-BFGS-B method for bounded optimization
mle <- optim(par = initial_values, fn = negloglik, samp = dat$x, method = "L-BFGS-B", lower = lower_bounds)

# Display the estimated parameters
print(mle$par)
```

```{r}
#The "optim" function may be more stable.
# optim(par=?,fn=?,samp=?)$par


```
What is the maximum likelihood estimate of the shape and scale parameters of the Weibull distribution for the sample in `dat_weibull.RData`?
-Shape parameter (shape): $0.9536652$
-Scale parameter (scale): $1.7886959$

# Question 3

## Context

Suppose $(S,M,P)_{\boldsymbol \theta}$ is a parametrized family of distributions. The parameter $\boldsymbol \theta$ may be vector-valued or one dimensional. Under fairly general circumstances, the maximum likelihood parameter estimate $\hat{\boldsymbol \theta}$ of the parameter $\boldsymbol \theta$ based on a sample $\{X_1,X_2,...X_n\}$ is *consistent*, also called *asymptotically consistent*. Informally, this means that as larger and larger samples are used to estimate the parameter, the estimate gets closer and closer to the true value. 

Some parameter estimates are *unbiased*. Informally, this means that if the estimate is applied to $M$ samples of size $n$ to get a collection of estimates $\left\{\hat{\boldsymbol \theta}_1,\hat{\boldsymbol \theta}_2,...\hat{\boldsymbol \theta}_M\right\}$, the mean of the estimates, $\frac{1}{M}\sum_{i=1}^M\hat{\boldsymbol \theta}_i$ will get closer and closer to $\hat{\boldsymbol \theta}$ as $M$ gets larger and larger.

The purpose of this question is to perform numerical experiments to gain insight into whether the maximum likelihood estimates of $\mu$ and $\sigma^2$ are unbiased for samples of size 5 from $Normal(0,1)$ 

Steps: 


A) Create a $10,000\times5$ matrix of samples of size 5 from the standard Normal distribution. 

```{r}
set.seed(45678)
mat<-matrix(rnorm(10000*5),ncol=5)
```

B) Use `apply` to calculate the maximum likelihood estimates $\hat{\mu}$ and $\hat{\sigma}^2$  of $\mu$ and $\sigma^2$ for each sample.

```{r}
mu.hat<-apply(mat,1,mean)
# function to calculate the maximum likelihood estimate of sigma^2 for a vector x
sigma2.ml<-function(x){sum((x-mean(x))^2)/5}
# your calculation of the estimates for sigma^2 here
sigma2.hat<-apply(mat,1,sigma2.ml) #
```

Compute and display the mean of the $\hat{\mu}$s and the mean of the $\hat{\sigma}^2$s.

```{r}
# your code here

# Calculate the mean of the estimated mus
mean_mu_hat <- mean(mu.hat)
cat("Mean of the estimated mus (mu.hat):", mean_mu_hat, "\n")

# Calculate the mean of the estimated sigma^2s
mean_sigma2_hat <- mean(sigma2.hat)
cat("Mean of the estimated sigma^2s (sigma2.hat):", mean_sigma2_hat, "\n")

```


## Q3, part 1

(5 points)

Compute and display the mean of the $\hat{\mu}$s.

```{r}
# your code here
# Compute and display the mean of the estimated mus (mu.hat)
mean_mu_hat <- mean(mu.hat)
cat("Mean of the estimated mus (mu.hat):", mean_mu_hat, "\n")

```

Does the maximum likelihood estimate of $\mu$ seem to be unbiased? (You may repeat the experiment with other seeds to help answer this question.)

The mean of the estimated $\hat{\mu}$s is $0.008816334$ which is very close to $0$, which is the true mean ($\mu$) of the standard Normal distribution from which the samples were drawn.This suggests that the maximum likelihood estimate (MLE) of $\mu$ for samples from a Normal distribution is unbiased.



## Q3, part 2

(5 points)

Compute and display the mean of the $\hat{\sigma}^2$s.

```{r}
# your code here
mean_sigma2_hat <- mean(sigma2.hat)
cat("Mean of the estimated sigma^2s (sigma2.hat):", mean_sigma2_hat, "\n")

# Adjusting the function to calculate the unbiased estimate of sigma^2
sigma2.unbiased <- function(x) {
  sum((x - mean(x))^2) / (length(x) - 1)
}

# Calculate the unbiased estimate of sigma^2 for each sample
sigma2.hat.unbiased <- apply(mat, 1, sigma2.unbiased)

# Compute and display the mean of the unbiased estimated sigma^2s
mean_sigma2_hat_unbiased <- mean(sigma2.hat.unbiased)
cat("Mean of the unbiased estimated sigma^2s (sigma2.hat.unbiased):", mean_sigma2_hat_unbiased, "\n")

```

Does the maximum likelihood estimate of $\sigma^2$ seem to be unbiased? (You may repeat the experiment with other seeds to help answer this question. Try comparing with the adjusted estimates produced by dividing the sum of the squared differences by 4 instead of 5.)

Given that the initial calculation where the maximum likelihood estimate (MLE) of $\sigma^2$ yielded a mean of  $0.8049875$, suggests that the MLE of $\sigma^2$, calculated as $\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2$ (with $n=5$ in this case), is biased for small samples. This is because the estimate systematically underestimates the true variance ($\sigma^2 = 1$) of the standard Normal distribution. The unbiased estimator of variance corrects for this by dividing by $n-1$ instead of $n$.Having the mean of the unbiased estimated $\sigma^2$s as  $1.006234$, indicates that adjusting the denominator to $n-1$ corrects the bias, making the estimate unbiased.



